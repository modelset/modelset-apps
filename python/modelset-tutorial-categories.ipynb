{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelSet - Basic tutorial\n",
    "\n",
    "ModelSet is a dataset of software models originally intented to help in the application of machine learning techniques to solve modelling tasks.\n",
    "\n",
    "In this tutorial we will explain how to load the dataset and extract basic features to perform a classification task: inferring the category of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First of all, you need to download and install ModelSet. \n",
    "\n",
    " 1. Download the package containing the raw models and the associated databases. Available at http://modelset.github.io/download/current.\n",
    " 2. Unzip the package\n",
    " 3. Make sure that the variable (see below) MODELSET_HOME points to the location in which you unzipped the package\n",
    " 4. Install the python library using pip\n",
    "    * If you have downloaded the source code of the library from http://github.com/modelset/modelset-py ,\n",
    "      then use `sys.path.append(\"/path/to/modelset-py/src\")` as a shortcut to load it dynamically.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do change this path to fit your local installation\n",
    "MODELSET_HOME=\"/home/jesus/projects/mde-ml/modelset/modelset-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The ModelSet library offers a convenient interface to dump the contents of the underlying database into a dataframe. In particular, there are several features available in the output dataframe:\n",
    "\n",
    " * The identifier of the model\n",
    " * The category of the model (manually labelled). Reflects the domain of the model.\n",
    " * Associated tags (zero or more manually labelled) which provide additional insights about the type of model.\n",
    " * The language of the model (typically english)\n",
    " * Basic stats. In the case of Ecore, number of elements, references, classes, attributes, packages, enumerations and datatypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import modelset.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.load(MODELSET_HOME, modeltype = 'ecore', selected_analysis = ['stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelset_df = dataset.to_normalized_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that all categories are defined\n",
    "modelset_df = modelset_df[~modelset_df['category'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# These dataframes are vectors\n",
    "ids     = modelset_df['id']\n",
    "labels  = modelset_df['category']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(ids, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train_filenames = [ dataset.txt_file(id) for id in train_X ]\n",
    "test_filenames  = [ dataset.txt_file(id) for id in test_X ]\n",
    "\n",
    "# max_iter=1000\n",
    "# stop_words = None, tokenizer = custom_tokenizer, min_df = 2\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='filename', min_df = 2)\n",
    "X = vectorizer.fit_transform(train_filenames)\n",
    "T = vectorizer.transform(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of the TF-IDF vectorization is a large matrix with len(train_X) rows and as many columns as words in the vocabulary\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#input_layer = X.shape[1]\n",
    "clf = MLPClassifier(solver='adam', learning_rate_init=0.01, hidden_layer_sizes=(64), random_state=1)\n",
    "clf.fit(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we evaluate the results obtained in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = clf.predict(X)\n",
    "# print(confusion_matrix(train_y, predict_train))\n",
    "train_report = classification_report(train_y, predict_train, output_dict = True)\n",
    "print(\"Training accuracy: \", train_report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we evaluate the classifier over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = clf.predict(T)\n",
    "test_report = classification_report(test_y, predict_test, output_dict = True)\n",
    "print(\"Training accuracy: \", test_report['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
